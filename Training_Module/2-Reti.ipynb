{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1' Addestramento Rete Feed-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Mean Squared Error before training: 3993652.75\n",
      "Epoch 1/200, Training Loss: 182601.5081655045, Validation Loss: 640.7804565429688\n",
      "Epoch 2/200, Training Loss: 267.1186880402143, Validation Loss: 216.2014923095703\n",
      "Epoch 3/200, Training Loss: 166.692339017811, Validation Loss: 157.46974182128906\n",
      "Epoch 4/200, Training Loss: 153.61122274817203, Validation Loss: 258.3143005371094\n",
      "Epoch 5/200, Training Loss: 146.65975865172575, Validation Loss: 503.9530029296875\n",
      "Epoch 6/200, Training Loss: 144.1117364732347, Validation Loss: 138.13839721679688\n",
      "Epoch 7/200, Training Loss: 141.1559369420193, Validation Loss: 124.92597198486328\n",
      "Epoch 8/200, Training Loss: 137.86764221175707, Validation Loss: 342.4842224121094\n",
      "Epoch 9/200, Training Loss: 135.62559521413334, Validation Loss: 156.5460662841797\n",
      "Epoch 10/200, Training Loss: 136.9560244379702, Validation Loss: 144.050537109375\n",
      "Epoch 11/200, Training Loss: 132.0970150914271, Validation Loss: 109.77495574951172\n",
      "Epoch 12/200, Training Loss: 132.28156315706448, Validation Loss: 125.39675903320312\n",
      "Epoch 13/200, Training Loss: 130.94315801399063, Validation Loss: 137.4407958984375\n",
      "Epoch 14/200, Training Loss: 130.1369082740194, Validation Loss: 120.0507583618164\n",
      "Epoch 15/200, Training Loss: 127.94007720375535, Validation Loss: 103.34224700927734\n",
      "Epoch 16/200, Training Loss: 127.23364623915903, Validation Loss: 118.50225830078125\n",
      "Epoch 17/200, Training Loss: 127.28349992160898, Validation Loss: 182.0751953125\n",
      "Epoch 18/200, Training Loss: 125.99506540302501, Validation Loss: 124.50194549560547\n",
      "Epoch 19/200, Training Loss: 126.90655340759135, Validation Loss: 327.8468933105469\n",
      "Epoch 20/200, Training Loss: 124.5775891543984, Validation Loss: 111.11316680908203\n",
      "Epoch 21/200, Training Loss: 125.70723372764556, Validation Loss: 120.45169830322266\n",
      "Epoch 22/200, Training Loss: 122.5111183645862, Validation Loss: 102.40147399902344\n",
      "Epoch 23/200, Training Loss: 122.30579663818358, Validation Loss: 99.00366973876953\n",
      "Epoch 24/200, Training Loss: 121.59786369740164, Validation Loss: 114.82408142089844\n",
      "Epoch 25/200, Training Loss: 120.03006560955627, Validation Loss: 104.54996490478516\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 161\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMean Squared Error before training:\u001b[39m\u001b[39m\"\u001b[39m, loss_before_training\u001b[39m.\u001b[39mitem())\n\u001b[1;32m    160\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m model, train_losses \u001b[39m=\u001b[39m train_model_regression(model, criterion, optimizer, num_epochs, train_loader, val_loader, device)\n\u001b[1;32m    163\u001b[0m file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfeedForwardNN.save\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    164\u001b[0m pickle\u001b[39m.\u001b[39mdump(model, file)\n",
      "Cell \u001b[0;32mIn[32], line 67\u001b[0m, in \u001b[0;36mtrain_model_regression\u001b[0;34m(model, criterion, optimizer, epoch, train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mfor\u001b[39;00m data, targets \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     65\u001b[0m     data, targets \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), targets\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 67\u001b[0m     optimizer\u001b[39m.\u001b[39;49mzero_grad()\n\u001b[1;32m     68\u001b[0m     y_pred \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     69\u001b[0m     loss \u001b[39m=\u001b[39m criterion(y_pred, targets)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_dynamo\u001b[39m.\u001b[39;49mdisable(fn, recursive)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_dynamo/eval_frame.py:489\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m     dynamo_config_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    488\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    490\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/optimizer.py:815\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    813\u001b[0m     per_device_and_dtype_grads \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 815\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_zero_grad_profile_name):\n\u001b[1;32m    816\u001b[0m     \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    817\u001b[0m         \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m group[\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/profiler.py:605\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__enter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 605\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecord \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49m_record_function_enter_new(\n\u001b[1;32m    606\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\n\u001b[1;32m    607\u001b[0m     )\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_ops.py:755\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    751\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    753\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    754\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 755\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m(kwargs \u001b[39mor\u001b[39;49;00m {}))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Define the Data Layer\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y).view(-1, 1)  # Reshape to (batch_size, 1) for regression\n",
    "\n",
    "        self.num_features = X.shape[1]\n",
    "        self.num_classes = 1  # Regression has only one output\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, :], self.y[idx]\n",
    "\n",
    "def feedforward_regression(input_size, hidden_size1, hidden_size2):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_size, hidden_size1),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size1, hidden_size2),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size2, 1)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define a function for the training process for regression\n",
    "def train_model_regression(model, criterion, optimizer, epoch, train_loader, val_loader, device):\n",
    "    n_iter = 0\n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses = []  # Lista per registrare le perdite durante l'addestramento\n",
    "\n",
    "    for epoch in range(epoch):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for data, targets in train_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(data)\n",
    "            loss = criterion(y_pred, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            n_iter += 1\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        average_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(average_loss)\n",
    "\n",
    "        labels, y_pred = test_model_regression(model, val_loader, device)\n",
    "        loss_val = criterion(y_pred, labels)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {average_loss}, Validation Loss: {loss_val.item()}\")\n",
    "\n",
    "    return model, train_losses\n",
    "\n",
    "# Define a function to evaluate the performance on validation and test sets for regression\n",
    "def test_model_regression(model, data_loader, device):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "\n",
    "    for data, targets in data_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        y_pred.append(model(data))\n",
    "        y_test.append(targets)\n",
    "\n",
    "    y_test = torch.cat(y_test)\n",
    "    y_pred = torch.cat(y_pred)\n",
    "\n",
    "    return y_test, y_pred\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "# Train hyperparameters\n",
    "num_epochs = 200  # Increase the number of epochs\n",
    "learning_rate = 0.0001  # Adjust the learning rate\n",
    "\n",
    "# Load data\n",
    "FILENAME = \"train.csv\"\n",
    "df = pd.read_csv(FILENAME)\n",
    "\n",
    "seed = 42\n",
    "\n",
    "# Select input variables (X) and output variable (y)\n",
    "X = df.drop(\"Year\", axis=1).values  # Convert to numpy array\n",
    "y = df[\"Year\"].values\n",
    "\n",
    "# Split data\n",
    "indices = np.arange(X.shape[0])\n",
    "train, test = train_test_split(indices, test_size=0.2, random_state=seed)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Scale data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X[train, :])\n",
    "X_val_scaled = scaler.transform(X[val, :])\n",
    "X_test_scaled = scaler.transform(X[test, :])\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = MyDataset(X_train_scaled, y[train])\n",
    "train_loader = DataLoader(train_dataset, shuffle=True)\n",
    "\n",
    "val_dataset = MyDataset(X_val_scaled, y[val])\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "\n",
    "test_dataset = MyDataset(X_test_scaled, y[test])\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "# Define the architecture, loss, and optimizer\n",
    "hidden_size1 = 64  # Increase the size of hidden layers\n",
    "hidden_size2 = 32\n",
    "model = feedforward_regression(train_dataset.num_features, hidden_size1, hidden_size2)\n",
    "model.to(device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  # Use Adam optimizer\n",
    "\n",
    "# Learning Rate Scheduling\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "# Test before training\n",
    "y_test, y_pred = test_model_regression(model, test_loader, device)\n",
    "loss_before_training = criterion(y_pred, y_test)\n",
    "print(\"Mean Squared Error before training:\", loss_before_training.item())\n",
    "\n",
    "# Train the model\n",
    "model, train_losses = train_model_regression(model, criterion, optimizer, num_epochs, train_loader, val_loader, device)\n",
    "\n",
    "torch.save(model, open(\"tabNet.save\", 'wb'))\n",
    "\n",
    "\n",
    "# Test after training\n",
    "y_test, y_pred = test_model_regression(model, test_loader, device)\n",
    "mse = mean_squared_error(y_test.detach().numpy(), y_pred.detach().numpy())\n",
    "mae = mean_absolute_error(y_test.detach().numpy(), y_pred.detach().numpy())\n",
    "mape = np.mean(np.abs((y_test.detach().numpy() - y_pred.detach().numpy()) / y_test.detach().numpy())) * 100\n",
    "r2 = r2_score(y_test.detach().numpy(), y_pred.detach().numpy())\n",
    "\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {mape}')\n",
    "print(f'R2 Score: {r2}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
