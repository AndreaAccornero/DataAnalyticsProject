{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1' Addestramento Rete Feed-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Mean Squared Error before training: 3994010.75\n",
      "Epoch 1/200, Training Loss: 413641.0206246307, Validation Loss: 8235.6416015625\n",
      "Epoch 2/200, Training Loss: 1417.719732465103, Validation Loss: 159.5406951904297\n",
      "Epoch 3/200, Training Loss: 151.33763747742594, Validation Loss: 105.64550018310547\n",
      "Epoch 4/200, Training Loss: 138.47325982519925, Validation Loss: 115.24968719482422\n",
      "Epoch 5/200, Training Loss: 131.21685945532704, Validation Loss: 116.29090881347656\n",
      "Epoch 6/200, Training Loss: 132.87336919479543, Validation Loss: 119.732177734375\n",
      "Epoch 7/200, Training Loss: 134.65481203973525, Validation Loss: 130.2337646484375\n",
      "Epoch 8/200, Training Loss: 130.50513331599618, Validation Loss: 96.36042785644531\n",
      "Epoch 9/200, Training Loss: 127.51351657265464, Validation Loss: 103.10466003417969\n",
      "Epoch 10/200, Training Loss: 126.50888801642118, Validation Loss: 127.1388931274414\n",
      "Epoch 11/200, Training Loss: 125.93974664280528, Validation Loss: 120.15068817138672\n",
      "Epoch 12/200, Training Loss: 124.42125707937443, Validation Loss: 117.11346435546875\n",
      "Epoch 13/200, Training Loss: 124.14563916027812, Validation Loss: 97.26264953613281\n",
      "Epoch 14/200, Training Loss: 123.99265740810495, Validation Loss: 103.37037658691406\n",
      "Epoch 15/200, Training Loss: 124.86856352420206, Validation Loss: 99.23655700683594\n",
      "Epoch 16/200, Training Loss: 122.91617451467988, Validation Loss: 97.09233093261719\n",
      "Epoch 17/200, Training Loss: 126.93522657248526, Validation Loss: 154.80885314941406\n",
      "Epoch 18/200, Training Loss: 120.5343239377485, Validation Loss: 91.60179138183594\n",
      "Epoch 19/200, Training Loss: 124.29648059926824, Validation Loss: 128.62232971191406\n",
      "Epoch 20/200, Training Loss: 127.02203102305397, Validation Loss: 160.4611053466797\n",
      "Epoch 21/200, Training Loss: 119.81385208817744, Validation Loss: 104.56642150878906\n",
      "Epoch 22/200, Training Loss: 119.84364859802795, Validation Loss: 124.0805892944336\n",
      "Epoch 23/200, Training Loss: 122.41852998740612, Validation Loss: 88.60842895507812\n",
      "Epoch 24/200, Training Loss: 116.07512597820502, Validation Loss: 99.57100677490234\n",
      "Epoch 25/200, Training Loss: 117.49451485054337, Validation Loss: 109.65872192382812\n",
      "Epoch 26/200, Training Loss: 118.13720041932395, Validation Loss: 114.8287124633789\n",
      "Epoch 27/200, Training Loss: 115.08999653600945, Validation Loss: 95.72821044921875\n",
      "Epoch 28/200, Training Loss: 115.36547671810207, Validation Loss: 120.16295623779297\n",
      "Epoch 29/200, Training Loss: 119.45437915049567, Validation Loss: 101.7401351928711\n",
      "Epoch 30/200, Training Loss: 117.93644236674497, Validation Loss: 95.6012191772461\n",
      "Epoch 31/200, Training Loss: 115.45333920832702, Validation Loss: 159.35511779785156\n",
      "Epoch 32/200, Training Loss: 116.39203059861217, Validation Loss: 117.32058715820312\n",
      "Epoch 33/200, Training Loss: 114.78764449258951, Validation Loss: 99.83969116210938\n",
      "Epoch 34/200, Training Loss: 115.80729854271766, Validation Loss: 112.31175994873047\n",
      "Epoch 35/200, Training Loss: 116.59218279573459, Validation Loss: 102.53384399414062\n",
      "Epoch 36/200, Training Loss: 114.62558874746398, Validation Loss: 89.73711395263672\n",
      "Epoch 37/200, Training Loss: 112.18188472742887, Validation Loss: 94.60489654541016\n",
      "Epoch 38/200, Training Loss: 113.5734066550271, Validation Loss: 100.20863342285156\n",
      "Epoch 39/200, Training Loss: 114.04454778545946, Validation Loss: 117.44178009033203\n",
      "Epoch 40/200, Training Loss: 115.02951550711794, Validation Loss: 149.267822265625\n",
      "Epoch 41/200, Training Loss: 114.32584875657204, Validation Loss: 99.39935302734375\n",
      "Epoch 42/200, Training Loss: 112.34735874066897, Validation Loss: 91.7515869140625\n",
      "Epoch 43/200, Training Loss: 112.99678350542018, Validation Loss: 94.89521026611328\n",
      "Epoch 44/200, Training Loss: 111.70475185385448, Validation Loss: 123.7450942993164\n",
      "Epoch 45/200, Training Loss: 111.33368737823818, Validation Loss: 92.82527923583984\n",
      "Epoch 46/200, Training Loss: 109.73527688236507, Validation Loss: 101.2613525390625\n",
      "Epoch 47/200, Training Loss: 111.3872881450525, Validation Loss: 110.41957092285156\n",
      "Epoch 48/200, Training Loss: 112.99446788418692, Validation Loss: 104.70397186279297\n",
      "Epoch 49/200, Training Loss: 110.38659196544288, Validation Loss: 104.85054016113281\n",
      "Epoch 50/200, Training Loss: 108.2944376814811, Validation Loss: 100.54363250732422\n",
      "Epoch 51/200, Training Loss: 112.02242211656409, Validation Loss: 87.5524673461914\n",
      "Epoch 52/200, Training Loss: 108.9564073361797, Validation Loss: 89.42448425292969\n",
      "Epoch 53/200, Training Loss: 106.51793560283772, Validation Loss: 97.17938232421875\n",
      "Epoch 54/200, Training Loss: 106.34317047037926, Validation Loss: 117.86456298828125\n",
      "Epoch 55/200, Training Loss: 109.081701428617, Validation Loss: 92.75\n",
      "Epoch 56/200, Training Loss: 105.77392533961309, Validation Loss: 112.88639831542969\n",
      "Epoch 57/200, Training Loss: 106.65240487314209, Validation Loss: 88.4982681274414\n",
      "Epoch 58/200, Training Loss: 105.7361113730845, Validation Loss: 102.22491455078125\n",
      "Epoch 59/200, Training Loss: 108.06652969508386, Validation Loss: 202.31947326660156\n",
      "Epoch 60/200, Training Loss: 104.92032317334294, Validation Loss: 100.67081451416016\n",
      "Epoch 61/200, Training Loss: 105.55654299311186, Validation Loss: 87.36176300048828\n",
      "Epoch 62/200, Training Loss: 109.76018097928463, Validation Loss: 79.03511047363281\n",
      "Epoch 63/200, Training Loss: 103.06873638103771, Validation Loss: 106.15438079833984\n",
      "Epoch 64/200, Training Loss: 103.89088465785252, Validation Loss: 116.0866470336914\n",
      "Epoch 65/200, Training Loss: 106.37213475447504, Validation Loss: 84.64530944824219\n",
      "Epoch 66/200, Training Loss: 104.22832003786836, Validation Loss: 98.46228790283203\n",
      "Epoch 67/200, Training Loss: 104.33347380899917, Validation Loss: 94.32743072509766\n",
      "Epoch 68/200, Training Loss: 104.78390643515577, Validation Loss: 87.32281494140625\n",
      "Epoch 69/200, Training Loss: 106.04832703265726, Validation Loss: 113.6144027709961\n",
      "Epoch 70/200, Training Loss: 101.43943591411302, Validation Loss: 95.1062240600586\n",
      "Epoch 71/200, Training Loss: 103.15160391117284, Validation Loss: 93.22460174560547\n",
      "Epoch 72/200, Training Loss: 102.81335705278224, Validation Loss: 83.14167785644531\n",
      "Epoch 73/200, Training Loss: 103.5259040841454, Validation Loss: 84.0838394165039\n",
      "Epoch 74/200, Training Loss: 103.2352976984201, Validation Loss: 88.707275390625\n",
      "Epoch 75/200, Training Loss: 102.62017316865864, Validation Loss: 80.986083984375\n",
      "Epoch 76/200, Training Loss: 102.32072553429481, Validation Loss: 137.3336639404297\n",
      "Epoch 77/200, Training Loss: 105.14210799289457, Validation Loss: 83.3495864868164\n",
      "Epoch 78/200, Training Loss: 99.97360598504443, Validation Loss: 87.14330291748047\n",
      "Epoch 79/200, Training Loss: 101.66013164159575, Validation Loss: 86.03717803955078\n",
      "Epoch 80/200, Training Loss: 101.24644773711034, Validation Loss: 88.12217712402344\n",
      "Epoch 81/200, Training Loss: 100.98030300584506, Validation Loss: 97.33634948730469\n",
      "Epoch 82/200, Training Loss: 100.49773207437157, Validation Loss: 86.3155288696289\n",
      "Epoch 83/200, Training Loss: 100.37600471923311, Validation Loss: 96.61534118652344\n",
      "Epoch 84/200, Training Loss: 99.06492016831203, Validation Loss: 92.5435562133789\n",
      "Epoch 85/200, Training Loss: 101.17854500640026, Validation Loss: 86.14175415039062\n",
      "Epoch 86/200, Training Loss: 100.76239313513906, Validation Loss: 89.68473052978516\n",
      "Epoch 87/200, Training Loss: 100.11989292070747, Validation Loss: 95.95209503173828\n",
      "Epoch 88/200, Training Loss: 99.31127602272633, Validation Loss: 98.92839813232422\n",
      "Epoch 89/200, Training Loss: 99.91606397149914, Validation Loss: 122.54830169677734\n",
      "Epoch 90/200, Training Loss: 100.22879288631943, Validation Loss: 83.22443389892578\n",
      "Epoch 91/200, Training Loss: 99.419653771866, Validation Loss: 88.07694244384766\n",
      "Epoch 92/200, Training Loss: 99.0236282121016, Validation Loss: 112.82898712158203\n",
      "Epoch 93/200, Training Loss: 100.18395921038106, Validation Loss: 93.99723815917969\n",
      "Epoch 94/200, Training Loss: 99.29201224981952, Validation Loss: 105.7370376586914\n",
      "Epoch 95/200, Training Loss: 99.28553601806034, Validation Loss: 92.43329620361328\n",
      "Epoch 96/200, Training Loss: 97.94840738718274, Validation Loss: 85.06945037841797\n",
      "Epoch 97/200, Training Loss: 98.26517089205997, Validation Loss: 86.11199951171875\n",
      "Epoch 98/200, Training Loss: 98.80852294761196, Validation Loss: 93.49400329589844\n",
      "Epoch 99/200, Training Loss: 99.84846044782724, Validation Loss: 83.22662353515625\n",
      "Epoch 100/200, Training Loss: 98.0052527808642, Validation Loss: 87.7622299194336\n",
      "Epoch 101/200, Training Loss: 99.20118223782914, Validation Loss: 85.71485900878906\n",
      "Epoch 102/200, Training Loss: 99.38020607100803, Validation Loss: 92.51018524169922\n",
      "Epoch 103/200, Training Loss: 99.25630287676414, Validation Loss: 82.58136749267578\n",
      "Epoch 104/200, Training Loss: 99.58688119256226, Validation Loss: 89.30142974853516\n",
      "Epoch 105/200, Training Loss: 98.17289587000595, Validation Loss: 86.96549987792969\n",
      "Epoch 106/200, Training Loss: 98.6605813093886, Validation Loss: 84.03531646728516\n",
      "Epoch 107/200, Training Loss: 98.30511854774382, Validation Loss: 150.9599151611328\n",
      "Epoch 108/200, Training Loss: 96.18970283286546, Validation Loss: 85.94486999511719\n",
      "Epoch 109/200, Training Loss: 100.07263714037862, Validation Loss: 135.1793212890625\n",
      "Epoch 110/200, Training Loss: 96.89450859807228, Validation Loss: 86.42471313476562\n",
      "Epoch 111/200, Training Loss: 97.19280625091645, Validation Loss: 91.14727783203125\n",
      "Epoch 112/200, Training Loss: 97.65190868288104, Validation Loss: 105.3462905883789\n",
      "Epoch 113/200, Training Loss: 98.24286002464925, Validation Loss: 89.11561584472656\n",
      "Epoch 114/200, Training Loss: 97.84855289717808, Validation Loss: 144.7639923095703\n",
      "Epoch 115/200, Training Loss: 96.54355476160664, Validation Loss: 106.21978759765625\n",
      "Epoch 116/200, Training Loss: 96.96765302028129, Validation Loss: 83.72901916503906\n",
      "Epoch 117/200, Training Loss: 98.46298466751499, Validation Loss: 85.8003158569336\n",
      "Epoch 118/200, Training Loss: 95.36516330074812, Validation Loss: 88.30244445800781\n",
      "Epoch 119/200, Training Loss: 97.63886902687322, Validation Loss: 81.70227813720703\n",
      "Epoch 120/200, Training Loss: 95.44450986722161, Validation Loss: 108.09625244140625\n",
      "Epoch 121/200, Training Loss: 97.47142593890408, Validation Loss: 94.0367431640625\n",
      "Epoch 122/200, Training Loss: 97.52095172962349, Validation Loss: 83.01907348632812\n",
      "Epoch 123/200, Training Loss: 97.25581485157717, Validation Loss: 93.28038787841797\n",
      "Epoch 124/200, Training Loss: 97.39938144418167, Validation Loss: 107.28592681884766\n",
      "Epoch 125/200, Training Loss: 95.3054511624379, Validation Loss: 83.96421813964844\n",
      "Epoch 126/200, Training Loss: 96.19344263650788, Validation Loss: 88.98563385009766\n",
      "Epoch 127/200, Training Loss: 96.2847617540438, Validation Loss: 90.80300903320312\n",
      "Epoch 128/200, Training Loss: 95.56872475336928, Validation Loss: 95.46940612792969\n",
      "Epoch 129/200, Training Loss: 96.184106146839, Validation Loss: 90.6101303100586\n",
      "Epoch 130/200, Training Loss: 97.38167727813737, Validation Loss: 82.62882232666016\n",
      "Epoch 131/200, Training Loss: 95.60171589477218, Validation Loss: 82.91891479492188\n",
      "Epoch 132/200, Training Loss: 95.98052698107296, Validation Loss: 99.13502502441406\n",
      "Epoch 133/200, Training Loss: 95.13593522405866, Validation Loss: 85.7591552734375\n",
      "Epoch 134/200, Training Loss: 96.81810831143763, Validation Loss: 85.5997085571289\n",
      "Epoch 135/200, Training Loss: 96.00161353626112, Validation Loss: 87.51547241210938\n",
      "Epoch 136/200, Training Loss: 96.0257229031261, Validation Loss: 82.25234985351562\n",
      "Epoch 137/200, Training Loss: 95.33492262734286, Validation Loss: 87.3450927734375\n",
      "Epoch 138/200, Training Loss: 94.55510067864122, Validation Loss: 95.42938995361328\n",
      "Epoch 139/200, Training Loss: 95.98698058913789, Validation Loss: 96.78829956054688\n",
      "Epoch 140/200, Training Loss: 95.86858620750688, Validation Loss: 89.26222229003906\n",
      "Epoch 141/200, Training Loss: 95.29325677061077, Validation Loss: 92.09603881835938\n",
      "Epoch 142/200, Training Loss: 95.68048618550554, Validation Loss: 98.17024230957031\n",
      "Epoch 143/200, Training Loss: 94.96299136511053, Validation Loss: 89.87583923339844\n",
      "Epoch 144/200, Training Loss: 94.54284910578032, Validation Loss: 117.34919738769531\n",
      "Epoch 145/200, Training Loss: 95.4531987031612, Validation Loss: 94.39330291748047\n",
      "Epoch 146/200, Training Loss: 94.44744078861517, Validation Loss: 90.27928161621094\n",
      "Epoch 147/200, Training Loss: 94.47818154769645, Validation Loss: 85.68649291992188\n",
      "Epoch 148/200, Training Loss: 95.74251913641446, Validation Loss: 91.88944244384766\n",
      "Epoch 149/200, Training Loss: 94.03841468457532, Validation Loss: 90.34515380859375\n",
      "Epoch 150/200, Training Loss: 94.81058290591618, Validation Loss: 91.5311508178711\n",
      "Epoch 151/200, Training Loss: 94.08705410167177, Validation Loss: 86.60310363769531\n",
      "Epoch 152/200, Training Loss: 93.85049887041252, Validation Loss: 90.5015640258789\n",
      "Epoch 153/200, Training Loss: 94.15226290469909, Validation Loss: 92.56038665771484\n",
      "Epoch 154/200, Training Loss: 94.61625778919715, Validation Loss: 105.34581756591797\n",
      "Epoch 155/200, Training Loss: 95.08065721752197, Validation Loss: 100.55264282226562\n",
      "Epoch 156/200, Training Loss: 93.38562198999179, Validation Loss: 86.53189849853516\n",
      "Epoch 157/200, Training Loss: 93.87355086541547, Validation Loss: 83.81494903564453\n",
      "Epoch 158/200, Training Loss: 93.3055825378054, Validation Loss: 88.60556030273438\n",
      "Epoch 159/200, Training Loss: 93.9822029062668, Validation Loss: 78.16436004638672\n",
      "Epoch 160/200, Training Loss: 92.57289611878427, Validation Loss: 97.09346008300781\n",
      "Epoch 161/200, Training Loss: 93.61486325390904, Validation Loss: 96.64892578125\n",
      "Epoch 162/200, Training Loss: 93.15891277302278, Validation Loss: 93.35872650146484\n",
      "Epoch 163/200, Training Loss: 93.91068824000314, Validation Loss: 81.64447784423828\n",
      "Epoch 164/200, Training Loss: 92.74667334536488, Validation Loss: 88.20465087890625\n",
      "Epoch 165/200, Training Loss: 93.36671787356791, Validation Loss: 94.70560455322266\n",
      "Epoch 166/200, Training Loss: 93.78372156406118, Validation Loss: 85.95244598388672\n",
      "Epoch 167/200, Training Loss: 93.09127909910032, Validation Loss: 88.62751770019531\n",
      "Epoch 168/200, Training Loss: 92.8432077891505, Validation Loss: 107.59062194824219\n",
      "Epoch 169/200, Training Loss: 92.55282258794792, Validation Loss: 87.94140625\n",
      "Epoch 170/200, Training Loss: 93.50828061010931, Validation Loss: 83.17938232421875\n",
      "Epoch 171/200, Training Loss: 93.26530666087538, Validation Loss: 87.31925964355469\n",
      "Epoch 172/200, Training Loss: 92.4957374551434, Validation Loss: 89.33583068847656\n",
      "Epoch 173/200, Training Loss: 92.06277915907175, Validation Loss: 86.32160186767578\n",
      "Epoch 174/200, Training Loss: 93.34785329551022, Validation Loss: 80.17930603027344\n",
      "Epoch 175/200, Training Loss: 92.51354041452437, Validation Loss: 85.86665344238281\n",
      "Epoch 176/200, Training Loss: 93.14714545747198, Validation Loss: 90.8790512084961\n",
      "Epoch 177/200, Training Loss: 91.99969466955513, Validation Loss: 86.70718383789062\n",
      "Epoch 178/200, Training Loss: 91.82471363615609, Validation Loss: 81.90320587158203\n",
      "Epoch 179/200, Training Loss: 92.7107485970858, Validation Loss: 92.24968719482422\n",
      "Epoch 180/200, Training Loss: 92.30564216731548, Validation Loss: 84.1331787109375\n",
      "Epoch 181/200, Training Loss: 91.59983434298472, Validation Loss: 146.12989807128906\n",
      "Epoch 182/200, Training Loss: 92.14518307155762, Validation Loss: 82.30089569091797\n",
      "Epoch 183/200, Training Loss: 92.30199280133318, Validation Loss: 84.42743682861328\n",
      "Epoch 184/200, Training Loss: 93.42263458631811, Validation Loss: 217.35992431640625\n",
      "Epoch 185/200, Training Loss: 91.04635212213891, Validation Loss: 103.03192901611328\n",
      "Epoch 186/200, Training Loss: 92.16698936524045, Validation Loss: 80.9417953491211\n",
      "Epoch 187/200, Training Loss: 90.57894123795367, Validation Loss: 86.96869659423828\n",
      "Epoch 188/200, Training Loss: 92.1848437611667, Validation Loss: 87.3284683227539\n",
      "Epoch 189/200, Training Loss: 90.94001250239492, Validation Loss: 92.30542755126953\n",
      "Epoch 190/200, Training Loss: 91.40038407684447, Validation Loss: 91.35562133789062\n",
      "Epoch 191/200, Training Loss: 92.23811747876057, Validation Loss: 82.23214721679688\n",
      "Epoch 192/200, Training Loss: 93.89030696301131, Validation Loss: 84.1138916015625\n",
      "Epoch 193/200, Training Loss: 90.76372173167651, Validation Loss: 84.97195434570312\n",
      "Epoch 194/200, Training Loss: 91.01176199666499, Validation Loss: 89.80244445800781\n",
      "Epoch 195/200, Training Loss: 90.79660616851578, Validation Loss: 95.89790344238281\n",
      "Epoch 196/200, Training Loss: 91.95562671714511, Validation Loss: 124.58037567138672\n",
      "Epoch 197/200, Training Loss: 90.06009788049677, Validation Loss: 86.13249969482422\n",
      "Epoch 198/200, Training Loss: 90.7087788620257, Validation Loss: 103.69811248779297\n",
      "Epoch 199/200, Training Loss: 91.20958946308296, Validation Loss: 88.43836975097656\n",
      "Epoch 200/200, Training Loss: 92.23788575036706, Validation Loss: 81.4287338256836\n",
      "Mean Squared Error (MSE): 83.85285186767578\n",
      "Mean Absolute Error (MAE): 6.430835247039795\n",
      "Mean Absolute Percentage Error (MAPE): 0.3228731220588088\n",
      "R2 Score: 0.23244753579323252\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Define the Data Layer\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y).view(-1, 1)  # Reshape to (batch_size, 1) for regression\n",
    "\n",
    "        self.num_features = X.shape[1]\n",
    "        self.num_classes = 1  # Regression has only one output\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, :], self.y[idx]\n",
    "\n",
    "def feedforward_regression(input_size, hidden_size1, hidden_size2):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_size, hidden_size1),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size1, 1)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define a function for the training process for regression\n",
    "def train_model_regression(model, criterion, optimizer, epoch, train_loader, val_loader, device):\n",
    "    n_iter = 0\n",
    "    train_losses = []  # Lista per registrare le perdite durante l'addestramento\n",
    "\n",
    "    for epoch in range(epoch):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for data, targets in train_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(data)\n",
    "            loss = criterion(y_pred, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            n_iter += 1\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        average_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(average_loss)\n",
    "\n",
    "        labels, y_pred = test_model_regression(model, val_loader, device)\n",
    "        loss_val = criterion(y_pred, labels)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {average_loss}, Validation Loss: {loss_val.item()}\")\n",
    "\n",
    "    return model, train_losses\n",
    "\n",
    "# Define a function to evaluate the performance on validation and test sets for regression\n",
    "def test_model_regression(model, data_loader, device):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "\n",
    "    for data, targets in data_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        y_pred.append(model(data))\n",
    "        y_test.append(targets)\n",
    "\n",
    "    y_test = torch.cat(y_test)\n",
    "    y_pred = torch.cat(y_pred)\n",
    "\n",
    "    return y_test, y_pred\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "# Train hyperparameters\n",
    "num_epochs = 200  # Increase the number of epochs\n",
    "learning_rate = 0.001\n",
    "batch = 8\n",
    "\n",
    "# Load data\n",
    "FILENAME = \"train.csv\"\n",
    "df = pd.read_csv(FILENAME)\n",
    "\n",
    "seed = 42\n",
    "\n",
    "# Select input variables (X) and output variable (y)\n",
    "X = df.drop(\"Year\", axis=1).values  # Convert to numpy array\n",
    "y = df[\"Year\"].values\n",
    "\n",
    "# Split data\n",
    "indices = np.arange(X.shape[0])\n",
    "train, test = train_test_split(indices, test_size=0.2, random_state=seed)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Scale data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X[train, :])\n",
    "X_val_scaled = scaler.transform(X[val, :])\n",
    "X_test_scaled = scaler.transform(X[test, :])\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = MyDataset(X_train_scaled, y[train])\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch)\n",
    "\n",
    "val_dataset = MyDataset(X_val_scaled, y[val])\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "\n",
    "test_dataset = MyDataset(X_test_scaled, y[test])\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "# Define the architecture, loss, and optimizer\n",
    "hidden_size1 = 64  # Increase the size of hidden layers\n",
    "hidden_size2 = 32\n",
    "model = feedforward_regression(train_dataset.num_features, hidden_size1, hidden_size2)\n",
    "model.to(device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  # Use Adam optimizer\n",
    "\n",
    "# Learning Rate Scheduling\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "# Test before training\n",
    "y_test, y_pred = test_model_regression(model, test_loader, device)\n",
    "loss_before_training = criterion(y_pred, y_test)\n",
    "print(\"Mean Squared Error before training:\", loss_before_training.item())\n",
    "\n",
    "# Train the model\n",
    "model, train_losses = train_model_regression(model, criterion, optimizer, num_epochs, train_loader, val_loader, device)\n",
    "\n",
    "torch.save(model, open(\"feedForwardNN.save\", 'wb'))\n",
    "\n",
    "# Test after training\n",
    "y_test, y_pred = test_model_regression(model, test_loader, device)\n",
    "mse = mean_squared_error(y_test.detach().numpy(), y_pred.detach().numpy())\n",
    "mae = mean_absolute_error(y_test.detach().numpy(), y_pred.detach().numpy())\n",
    "mape = np.mean(np.abs((y_test.detach().numpy() - y_pred.detach().numpy()) / y_test.detach().numpy())) * 100\n",
    "r2 = r2_score(y_test.detach().numpy(), y_pred.detach().numpy())\n",
    "\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {mape}')\n",
    "print(f'R2 Score: {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For reproducibility\\n\",\n",
    "def fix_random(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Define the Data Layer\\n\",\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y).view(-1, 1)  # Reshape to (batch_size, 1) for regression\\n\",\n",
    "         \n",
    "        self.num_features = X.shape[1]\n",
    "        self.num_classes = 1  # Regression has only one output\\n\",\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, :], self.y[idx]\n",
    "\n",
    "# Define a simple neural network for regression\\n\",\n",
    "class FeedForwardRegression(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(FeedForwardRegression, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.fc2 = nn.Linear(self.hidden_size, 1)  # Output layer has 1 neuron for regression\\n\",\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.fc1(x)\n",
    "        h = self.relu(h)\n",
    "        output = self.fc2(h)\n",
    "        return output\n",
    "\n",
    "# Define a function for the training process for regression\\n\",\n",
    "def train_model_regression(model, criterion, optimizer, epoch, train_loader, val_loader, device, writer, log_name=\"model\"):\n",
    "    n_iter = 0\n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses = []  # Lista per registrare le perdite durante l'addestramento\\n\",\n",
    "\n",
    "    for epoch in range(epoch):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for data, targets in train_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(data)\n",
    "            loss = criterion(y_pred, targets)\n",
    "            writer.add_scalar(\"Loss/train\\\", loss, n_iter\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            n_iter += 1\\\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        average_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(average_loss)\n",
    "\n",
    "        labels, y_pred = test_model_regression(model, val_loader, device)\n",
    "        loss_val = criterion(y_pred, labels)\n",
    "        writer.add_scalar(\"Loss/val\\\", loss_val, epoch\")\n",
    "        \n",
    "        if loss_val.item() < best_valid_loss:\n",
    "            best_valid_loss = loss_val.item()\n",
    "            if not os.path.exists('models'):\n",
    "                os.makedirs('models')\n",
    "            torch.save(model.state_dict(), 'models/'+log_name)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {average_loss}, Validation Loss: {loss_val.item()}\")\n",
    "\n",
    "    return model, train_losses\\\n",
    "\n",
    "# Define a function to evaluate the performance on validation and test sets for regression\\n\",\n",
    "def test_model_regression(model, data_loader, device):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "\n",
    "    for data, targets in data_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        y_pred.append(model(data))\n",
    "        y_test.append(targets)\n",
    "\n",
    "    y_test = torch.cat(y_test)\n",
    "    y_pred = torch.cat(y_pred)\n",
    "\n",
    "    return y_test, y_pred\n",
    "\n",
    "# Set device\\n\",\n",
    "device = torch.device('cpu')  # or 'cuda' if available\\n\",\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "# Train hyperparameters\\n\",\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "batch = 16\n",
    "\n",
    "# Load data\\n\",\n",
    "FILENAME = \"train.csv\",\n",
    "df = pd.read_csv(FILENAME)\n",
    "\n",
    "seed = 42\n",
    "\n",
    "# Select input variables (X) and output variable (y)\n",
    "X = df.drop(\"Year\", axis=1).values  # Convert to numpy array\n",
    "y = df[\"Year\"].values\\\n",
    "\n",
    "# Split data\\n\",\n",
    "indices = np.arange(X.shape[0])\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=seed)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Create dataset\\n\",\n",
    "my_dataset = MyDataset(X, y)\n",
    "\n",
    "# Create subsets and relative dataloaders\\n\",\n",
    "train_subset = Subset(my_dataset, train_idx)\n",
    "train_loader = DataLoader(train_subset, batch_size=batch, shuffle=True)\n",
    "\n",
    "val_subset = Subset(my_dataset, val_idx),\n",
    "val_loader = DataLoader(val_subset, batch_size=1)\n",
    "\n",
    "test_subset = Subset(my_dataset, test_idx)\n",
    "test_loader = DataLoader(test_subset, batch_size=1)\n",
    "\n",
    "# Set seed for reproducibility\",\n",
    "fix_random(seed)\n",
    "\n",
    "\n",
    "# Define the architecture, loss, and optimizer\\n\",\n",
    "hidden_size = 32\n",
    "model = FeedForwardRegression(my_dataset.num_features, hidden_size)\n",
    "model.to(device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Test before training\\n\",\n",
    "y_test, y_pred = test_model_regression(model, test_loader, device)\n",
    "loss_before_training = criterion(y_pred, y_test)\n",
    "print(\"Mean Squared Error before training:\", loss_before_training.item())\n",
    "\n",
    "# Train the model \\n\",\n",
    "trained_model, train_losses = train_model_regression(model, criterion, optimizer, num_epochs, train_loader, val_loader, device, writer)\\n\",\n",
    "\n",
    "# Save the best model\\n\",\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "torch.save(trained_model.state_dict(), 'models/best_model.pth')\n",
    "\n",
    "# Load the best model\\n\",\n",
    "best_model_path = 'models/best_model.pth'\n",
    "if os.path.exists(best_model_path):\n",
    "    trained_model.load_state_dict(torch.load(best_model_path))\n",
    "    trained_model.to(device)\n",
    "    print(\"Model loaded successfully.\")\n",
    "else:\n",
    "    print(\"Error: The specified model path does not exist.\")\n",
    "\n",
    "# Test after training\n",
    "y_test, y_pred = test_model_regression(trained_model, test_loader, device)\n",
    "loss_after_training = criterion(y_pred, y_test)\n",
    "print(\"Mean Squared Error after training:\", loss_after_training.item())\n",
    "\n",
    "# Visualizza l'andamento delle perdite durante l'addestramento\\n\",\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
